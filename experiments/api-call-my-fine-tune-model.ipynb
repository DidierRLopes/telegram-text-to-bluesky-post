{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilize fine-tune model\n",
    "\n",
    "There are three main approaches to this:\n",
    "\n",
    "1. Download a model from HuggingFace directly, in my case that would be: didierlopes/phi-3-mini-4k-instruct-ft-on-didier-blog\n",
    "\n",
    "2. Load a base model from HuggingFace (e.g. microsoft/Phi-3-mini-4k-instruct) with the LoRA adapters from the fine-tuning which exist locally on the machine - using MLX\n",
    "\n",
    "3. Load a local fused model (base model + LoRA adapters) - using MLX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Run model off HuggingFace\n",
    "\n",
    "The assumption is that you have pushed your fine-tuned model to HuggingFace.\n",
    "\n",
    "```\n",
    "pip install torch torchvision torchaudio\n",
    "pip install transformers\n",
    "pip install 'accelerate>=0.26.0'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/didierlopes/miniconda3/envs/bsky/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set up the model and tokenizer\n",
    "model_name = \"didierlopes/phi-3-mini-4k-instruct-ft-on-didier-blog\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Define a function to generate text\n",
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are cached in: /Users/didierlopes/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils import TRANSFORMERS_CACHE\n",
    "\n",
    "# Check if the model is stored in cache\n",
    "print(f\"Models are cached in: {TRANSFORMERS_CACHE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the OpenBB workspace? How is it different from others?\n",
      "The OpenBB workspace is a collaborative platform for finance professionals, powered by our AI engine, designed to streamline workflows and improve productivity. It's unique in its ability to integrate with multiple financial tools and data sources, allowing users to create custom workflows and pipelines. Our workspace also offers advanced features such as natural language processing and machine learning to enhance decision-making\n"
     ]
    }
   ],
   "source": [
    "# Provide a sample prompt and generate output\n",
    "generated_output = generate_text(\n",
    "    \"What is the OpenBB workspace? How is it different from others?\"\n",
    ")\n",
    "\n",
    "print(generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load base model of HuggingFace with local LoRA adapters (using MLX)\n",
    "\n",
    "```\n",
    "conda install -c conda-forge mlx\n",
    "CONDA_SUBDIR=osx-arm64 conda create -n bsky python=3.11\n",
    "conda activate bsky\n",
    "conda config --env --set subdir osx-arm64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 203455.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The OpenBB workspace is a collaborative platform for financial analysts to share insights, data, and tools. It's different from others because it's built on open-source technology, allowing users to customize and extend the platform to fit their needs. This openness fosters a community-driven approach to financial analysis.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model_lora, tokenizer_lora = load(\n",
    "    model_path,\n",
    "    adapter_path=\"../../fine-tune-llm/adapters\"\n",
    ")\n",
    "\n",
    "output = generate(\n",
    "    model_lora,\n",
    "    tokenizer_lora,\n",
    "    \"What is the OpenBB workspace? How is it different from others?\",\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load fused model locally (using MLX)\n",
    "\n",
    "```\n",
    "CONDA_SUBDIR=osx-arm64 conda create -n fine-tune-llm python=3.11\n",
    "conda activate fine-tune-llm\n",
    "conda config --env --set subdir osx-arm64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The OpenBB workspace is a collaborative platform that allows users to share and discuss financial data, research, and insights. It's different from others in that it's built on top of the OpenBB Terminal, providing a seamless integration between the terminal and the workspace. This allows users to easily access and share data, making it a powerful tool for financial analysis and decision-making.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "fused_model, fused_tokenizer = load(\"../../fine-tune-llm/lora_fused_model\")\n",
    "\n",
    "output = generate(\n",
    "    fused_model,\n",
    "    fused_tokenizer,\n",
    "    \"What is the OpenBB workspace? How is it different from others?\",\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bsky",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
